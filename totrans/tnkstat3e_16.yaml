- en: Analytic Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://allendowney.github.io/ThinkStats/chap14.html](https://allendowney.github.io/ThinkStats/chap14.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This book has focused on computational methods like simulation and resampling,
    but some of the problems we solved have analytic solutions that can be much faster
    to compute.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter presents some of these methods and explains how they work. At the
    end of the chapter, I make suggestions for integrating computational and analytic
    methods for data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to run this notebook on Colab](https://colab.research.google.com/github/AllenDowney/ThinkStats/blob/v3/nb/chap14.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Normal Probability Plots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many analytic methods are based on the properties of the normal distribution,
    for two reasons: distributions of many measurements in the real world are well-approximated
    by normal distributions, and normal distributions have mathematical properties
    that make them useful for analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate the first point, we’ll look at some of the measurements in the
    penguin dataset. Then we’ll explore the mathematical properties of the normal
    distribution. Instructions for downloading the data are in the notebook for this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The following cell downloads the data from a repository created by Allison Horst.
  prefs: []
  type: TYPE_NORMAL
- en: 'Horst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica)
    penguin data. R package version 0.1.0\. [https://allisonhorst.github.io/palmerpenguins/](https://allisonhorst.github.io/palmerpenguins/).
    doi: 10.5281/zenodo.3960218.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data was collected as part of the research that led to this paper: Gorman
    KB, Williams TD, Fraser WR (2014). Ecological sexual dimorphism and environmental
    variability within a community of Antarctic penguins (genus Pygoscelis). PLoS
    ONE 9(3):e90081\. [https://doi.org/10.1371/journal.pone.0090081](https://doi.org/10.1371/journal.pone.0090081)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We can read the data like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The dataset contains measurements from three penguin species. For this example,
    we’ll select the Adélie penguins.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: To see if penguin weights follow a normal distribution, we’ll compute the empirical
    CDF of the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: And we’ll compute the analytic CDF of a normal distribution with the same mean
    and standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here’s what the CDF of the data looks like compared to the normal model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/9a8e665b64947ce1df5c9f8f7bde385b1fbfeff8d4d5872fbf739f23334e2197.png](../Images/62ff50eeea4aad9d47284c707539dfd2.png)'
  prefs: []
  type: TYPE_IMG
- en: The normal distribution might be a good enough model of this data, but it’s
    certainly not a perfect fit.
  prefs: []
  type: TYPE_NORMAL
- en: In general, plotting the CDF of the data and the CDF of a model is a good way
    to evaluate how well the model fits the data. But one drawback of this method
    is that it depends on how well we estimate the parameters of the model – in this
    example, the mean and standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative is a **normal probability plot**, which does not depend on our
    ability to estimate parameters. In a normal probability plot the \(y\) values
    are the sorted measurements.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: And the \(x\) values are the corresponding percentiles of a normal distribution,
    computed using the `ppf` method of the `norm` object, which computes the “percent
    point function”, which is the inverse CDF.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: If the measurements are actually drawn from a normal distribution, the \(y\)
    and \(x\) values should fall on a straight line. To see how well they do, we can
    use `linregress` to fit a line.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The following figure shows the \(x\) and \(y\) values along with the fitted
    line.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/ee75bd9cc7810d5af27c91f274b525a5360c747772232f8e1cda9fd37d207875.png](../Images/458e42ad190b4824acaca1953d4bd2c9.png)'
  prefs: []
  type: TYPE_IMG
- en: The normal probability plot is not a perfectly straight line, which indicates
    that the normal distribution is not a perfect model for this data.
  prefs: []
  type: TYPE_NORMAL
- en: One reason is that the dataset includes male and female penguins, and the two
    groups have different means – let’s see what happens if we plot the groups separately.
    The following function encapsulates the steps we used to make a normal probability
    plot.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Here’s what the results look like for male and female penguins separately.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/08b4e496844a147578d0273f68610e8a9cb0e2ed5d4ad465e5e3c2bba9fe6c46.png](../Images/9f9c2a9ad5ded5805244678fd3320af9.png)'
  prefs: []
  type: TYPE_IMG
- en: The normal probability plots for both groups are close to a straight line, which
    indicates that the distributions of weight follow normal distributions. When we
    put the groups together, the distribution of their weights is a mixture of two
    normal distributions with different means – and a mixture like that is not always
    well modeled by a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s consider some of the mathematical properties of normal distributions
    that make them so useful for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Normal Distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following class defines an object that represents a normal distribution.
    It contains as attributes the parameters `mu` and `sigma2`, which represent the
    mean and variance of the distribution. The name `sigma2` is a reminder that variance
    is the square of the standard deviation, which is usually denoted `sigma`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As an example, we’ll create a `Normal` object that represents a normal distribution
    with the same mean and variance as the weights of the male penguins.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: And another `Normal` object with the same mean and variance as the weights of
    the female penguins.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Next we’ll add a method to the `Normal` class that generates a random sample
    from a normal distribution. To add methods to an existing class, we’ll use a Jupyter
    magic command, `add_method_to`, which is defined in the `thinkstats` module. This
    command is not part of Python – it only works in Jupyter notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll use `sample` to demonstrate the first useful property of a normal distribution:
    if you draw values from two normal distributions and add them, the distribution
    of the sum is also normal.'
  prefs: []
  type: TYPE_NORMAL
- en: As an example, we’ll generate samples from the `Normal` objects we just made,
    add them together, and make a normal probability plot of the sums.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/33b567cbcfadb8b65eac37a8651c71cadf7ba8050819c06189d16430ccc37478.png](../Images/06575c803a0393e6266f6ff63498af9e.png)'
  prefs: []
  type: TYPE_IMG
- en: The normal probability plot looks like a straight line, which indicates that
    the sums follow a normal distribution. And that’s not all – if we know the parameters
    of the two distributions, we can compute the parameters of the distribution of
    the sum. The following method shows how.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In the distribution of the sum, the mean is the sum of the means and the variance
    is the sum of the variances. Now that we’ve defined the special method `__add__`,
    we can use the `+` operator to “add” two distributions – that is, to compute the
    distribution of their sum.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: To confirm that this result is correct, we’ll use the following method, which
    plots the analytic CDF of a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Here’s the result along with the empirical CDF of the sum of the random samples.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/735bf59b3c32e52a9510e4cd08aa8a167823e52e9fedda2778932a3b80014ab5.png](../Images/659441ee652a9e4b945bb30ea0117327.png)'
  prefs: []
  type: TYPE_IMG
- en: It looks like the parameters we computed are correct, which confirms that we
    can add two normal distributions by adding their means and variances.
  prefs: []
  type: TYPE_NORMAL
- en: As a corollary, if we generate `n` values from a normal distribution and add
    them up, the distribution of the sum is also a normal distribution. To demonstrate,
    we’ll start by generating 73 values from the distribution of male weights and
    adding them up. The following loop does that 1001 times, so the result is a sample
    from the distribution of sums.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The following method makes a `Normal` object that represents the distribution
    of the sums. To compute the parameters, we multiply both the mean and variance
    by `n`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Here’s the distribution of the sum of `n` weights.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: And here’s how it compares to the empirical distribution of the random sample.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/6c835d9609c5a715253f46fd2a9d60deadda0df552e1532d4ae1b5c4da9ef7b1.png](../Images/6e6afabc5ad7f5b9f61627e2cf39fa2e.png)'
  prefs: []
  type: TYPE_IMG
- en: The analytic distribution fits the distribution of the sample, which confirms
    that the `sum` method is correct. So if we collect a sample of `n` measurements,
    we can compute the distribution of their sum.
  prefs: []
  type: TYPE_NORMAL
- en: Distribution of Sample Means
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we can compute the distribution of a sample sum, we can also compute the
    distribution of a sample mean. To do that, we’ll use a third property of a normal
    distribution: if we multiply or divide by a constant, the result is a normal distribution.
    The following methods show how we compute the parameters of the distribution of
    a product or quotient.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: To compute the distribution of the product we multiply the mean by `factor`
    and the variance by the square of `factor`. We can use this property to compute
    the distribution of the sample means.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: To see if the result is correct, we’ll also compute the means of the random
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: And compare the normal model to the empirical CDF of the sample means.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/7d88dfb12c211ddbfd036362ac882a0b5e5036a574f1c85b5564ca4c0c6f7cb2.png](../Images/9e87e022efe163576039815838ea036c.png)'
  prefs: []
  type: TYPE_IMG
- en: The model and the simulation results agree, which shows that we can compute
    the distribution of the sample means analytically – which is very fast, compared
    to resampling.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know the sampling distribution of the mean, we can use it to compute
    the standard error, which is the standard deviation of the sampling distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This result suggests a shortcut we can use to compute the standard error directly,
    without computing the sampling distribution. In the sequence of steps we followed,
    we multiplied the variance by `n` and then divided by `n**2` – the net effect
    was to divide the variance by `n`, which means we divided the standard deviation
    by the square root of `n`.
  prefs: []
  type: TYPE_NORMAL
- en: So we can compute the standard error of the sample mean like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Now let’s consider one more result we can compute with normal distributions,
    the distribution of differences.
  prefs: []
  type: TYPE_NORMAL
- en: Distribution of Differences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Putting together the steps from the previous section, here’s how we can compute
    the distribution of sample means for the weights of the female penguins.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Now we have sampling distributions for the average weight of male and female
    penguins – let’s compute the distribution of the differences. The following method
    computes the distribution of the difference between values from two normal distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'As you might expect, the mean of the differences is the difference of the means.
    But as you might not expect, the variance of the differences is not the difference
    of the variances – it’s the sum! To see why, imagine we perform subtraction in
    two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: If we negate the second distribution, the mean is negated but the variance is
    the same.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then if we add in the first distribution, the variance of the sum is the sum
    of the variances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If that doesn’t convince you, let’s test it. Here’s the analytic distribution
    of the differences.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: And here’s a random sample of differences.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The following figure shows the empirical CDF of the random sample and the analytic
    CDF of the normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/596c267b5ce8ac93fd569603318b1d2f34e2fa6ccce9cba0b17d308d1d531d23.png](../Images/c310c26a18ca847fd83d5d6e8789376a.png)'
  prefs: []
  type: TYPE_IMG
- en: They agree, which confirms that we found the distribution of the differences
    correctly. We can use this distribution to compute a confidence interval for the
    difference in weights. We’ll use the following method to compute the inverse CDF.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The 5th and 95th percentiles form a 90% confidence interval.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: We get approximately the same results from the random sample.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The analytic method is faster than resampling, and it is deterministic – that
    is, not random.
  prefs: []
  type: TYPE_NORMAL
- en: However, everything we’ve done so far is based on the assumption that the distribution
    of measurements is normal. That’s not always true – in fact, with real data it
    is never exactly true. But even if the distribution of the measurements isn’t
    normal, if we add up many measurements, the distribution of their sum is often
    close to normal. That is the power of the Central Limit Theorem.
  prefs: []
  type: TYPE_NORMAL
- en: '## Central Limit Theorem'
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in the previous sections, if we add values drawn from normal distributions,
    the distribution of the sum is normal. Most other distributions don’t have this
    property – for example, if we add values drawn from an exponential distribution,
    the distribution of the sum is not exponential.
  prefs: []
  type: TYPE_NORMAL
- en: But for many distributions, if we generate `n` values and add them up, the distribution
    of the sum converges to normal as `n` increases. More specifically, if the distribution
    of the values has mean `m` and variance `s2` the distribution of the sum converges
    to a normal distribution with mean `n * m` and variance `n * s2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'That conclusion is the Central Limit Theorem (CLT). It is one of the most useful
    tools for statistical analysis, but it comes with caveats:'
  prefs: []
  type: TYPE_NORMAL
- en: The values have to come from the same distribution (although this requirement
    can be relaxed).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The values have to be drawn independently. If they are correlated, the CLT doesn’t
    apply (although it can still work if the correlation is not too strong).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The values have to be drawn from a distribution with finite mean and variance.
    So the CLT doesn’t apply to some long-tailed distributions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Central Limit Theorem explains the prevalence of normal distributions in
    the natural world. Many characteristics of living things are affected by genetic
    and environmental factors whose effect is additive. The characteristics we measure
    are the sum of a large number of small effects, so their distribution tends to
    be normal.
  prefs: []
  type: TYPE_NORMAL
- en: To see how the Central Limit Theorem works, and when it doesn’t, let’s try some
    experiments, starting with an exponential distribution. The following loop generates
    samples from an exponential distribution, adds them up, and makes a dictionary
    that maps from each sample size, `n`, to a list of 1001 sums.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Here are the averages for each list of sums.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: The average value from this distribution is 1, so if we add up 10 values, the
    average of the sum is close to 10, and if we add up 100 values the average of
    the sum is close to 100.
  prefs: []
  type: TYPE_NORMAL
- en: This function takes the `DataFrame` we just made and makes a normal probability
    plot for each list of sums.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: The following figure shows normal probability plots for the three lists of sums
    (the definition of `normal_plot_samples` is in the notebook for this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/771475bfc28457c8034dd0715e4bc6d6dcd8e75838c03bfb8a49aaa1d3c4ae23.png](../Images/0108d89c99b952ef6e24398ee1d8d6b0.png)'
  prefs: []
  type: TYPE_IMG
- en: When `n=1`, the distribution of the sum is exponential, so the normal probability
    plot is not a straight line. But with `n=10` the distribution of the sum is approximately
    normal, and with `n=100` it is almost indistinguishable from normal.
  prefs: []
  type: TYPE_NORMAL
- en: For distributions that are less skewed than an exponential, the distribution
    of the sum converges to normal more quickly – that is, for smaller values of `n`.
    For distributions that are more skewed, it takes longer. As an example, let’s
    look at sums of values from a lognormal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Here are the normal probability plots for the same range of sample sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/992059cdf6c2ff8c2aefa522b20aed972f30ffed9f8a5c38f86104666289e8fd.png](../Images/9018fbb8d29d90d22a11db5638999d03.png)'
  prefs: []
  type: TYPE_IMG
- en: When `n=1`, a normal model does not fit the distribution, and it is not much
    better with `n=10`. Even with `n=100`, the tails of the distribution clearly deviate
    from the model.
  prefs: []
  type: TYPE_NORMAL
- en: The mean and variance of the lognormal distribution are finite, so the distribution
    of the sum converges to normal eventually. But for some highly skewed distributions,
    it might not converge at any practical sample size. And in some cases, it doesn’t
    happen at all.
  prefs: []
  type: TYPE_NORMAL
- en: The Limits of the Central Limit Theorem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pareto distributions are even more skewed than lognormal. Depending on the parameters,
    some Pareto distributions do not have finite mean and variance – in those cases,
    the Central Limit Theorem does not apply.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate, we’ll generate values from a Pareto distribution with parameter
    `alpha=1`, which has infinite mean and variance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Here’s what the normal probability plots look like for a range of sample sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/587b9641385d2eb3b402badb3ed3172d2a45694e547a64cf0a6f2c63806aac36.png](../Images/19722c4ba77224c2d4d07bbf65b4cef3.png)'
  prefs: []
  type: TYPE_IMG
- en: Even with `n=100`, the distribution of the sum is nothing like a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: I also mentioned that the CLT does not apply if the values are correlated. To
    test that, we’ll use a function called `generate_expo_correlated` to generate
    values from an exponential distribution where the serial correlation – that is,
    the correlation between successive elements in the sample – is the given value,
    `rho`. This function is defined in the notebook for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Given a correlated sequence from a normal distribution, the following function
    generates a correlated sequence from an exponential distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: It starts with a sequence of correlated normal values and uses the normal CDF
    to transform them to a sequence of values from a uniform distribution between
    0 and 1. Then it uses the exponential inverse CDF to transform them to a sequence
    of exponential values.
  prefs: []
  type: TYPE_NORMAL
- en: The following loop makes a `DataFrame` with one column for each sample size
    and 1001 sums in each column.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Here are the normal probability plots for the distribution of these sums.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/14c589bb074224b2acca7b61ea39e860ba0bd6970265d02e1629e183da8b0519.png](../Images/067ebdb9bf70407ca9ec4720b401c3b5.png)'
  prefs: []
  type: TYPE_IMG
- en: With `rho=0.8`, there is a strong correlation between successive elements, and
    the distribution of the sum converges slowly. If there is also a strong correlation
    between distant elements of the sequence, it might not converge at all.
  prefs: []
  type: TYPE_NORMAL
- en: The previous section shows that the Central Limit Theorem works, and this section
    shows what happens when it doesn’t. Now let’s see how we can use it.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the CLT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To see why the Central Limit Theorem is useful, let’s get back to the example
    in [Chapter 9](chap09.html#section-diff-means): testing the apparent difference
    in mean pregnancy length for first babies and others. We’ll use the NSFG data
    again – instructions for downloading it are in the notebook for this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: The following cell downloads the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: We’ll use `get_nsfg_groups` to read the data and divide it into first babies
    and others.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: As we’ve seen, first babies are born a little later, on average – the apparent
    difference is about 0.078 weeks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: To see whether this difference might have happened by chance, we’ll assume as
    a null hypothesis that the mean and variance of pregnancy lengths is actually
    the same for both groups, so we can estimate it using all live births.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: The distribution of pregnancy lengths does not follow a normal distribution
    – nevertheless, we can use a normal distribution to approximate the sampling distribution
    of the mean.
  prefs: []
  type: TYPE_NORMAL
- en: The following function takes a sequence of values and returns a `Normal` object
    that represents the sampling distribution of the mean of a sample with the given
    size, `n`, drawn from a normal distribution with the same mean and variance as
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Here’s a normal approximation to the sampling distribution of mean weight for
    first births, under the null hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: And here’s the sampling distribution for other babies.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: We can compute the sampling distribution of the difference like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: The mean is 0, which makes sense because if we draw two samples from the same
    distribution, we expect the difference in means to be 0, on average. The variance
    of the sampling distribution is 0.0032, which indicates how much variation we
    expect in the difference due to chance.
  prefs: []
  type: TYPE_NORMAL
- en: To confirm that this distribution approximates the sampling distribution, we
    can also estimate it by resampling.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: Here’s the empirical CDF of the resampled differences compared to the normal
    model. The vertical dotted lines show the observed difference, positive and negative.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/8554f3cc4200ae56cb83ba2865c3fa27e114ce327b2076f6c9c7e765e5bc586d.png](../Images/0b6ab9aed3b56feeaab7266d87d8a99c.png)'
  prefs: []
  type: TYPE_IMG
- en: In this example, the sample sizes are large and the skewness of the measurements
    is modest, so the sampling distribution is well approximated by a normal distribution.
    Therefore, we can use the normal CDF to compute a p-value. The following method
    computes the CDF of a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: Here’s the probability of a difference as large as `delta` under the null hypothesis,
    which is the area under the right tail of the sampling distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: And here’s the probability of a difference as negative as `-delta`, which is
    the area under the left tail.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '`left` and `right` are the same because the normal distribution is symmetric.
    The sum of the two is the probability of a difference as large as `delta`, positive
    or negative.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: The resulting p-value is 0.170, which is consistent with the estimate we computed
    by resampling in [Chapter 9](chap09.html#section-diff-means).
  prefs: []
  type: TYPE_NORMAL
- en: The way we computed this p-value is similar to an **independent sample \(t\)
    test**. SciPy provides a function called `ttest_ind` that takes two samples and
    computes a p-value for the difference in their means.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: When the sample sizes are large, the result of the \(t\) test is close to what
    we computed with normal distributions. The \(t\) test is so called because it
    is based on a **t distribution** rather than a normal distribution. The \(t\)
    distribution is also useful for testing whether a correlation is statistically
    significant, as we’ll see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation Test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 9](chap09.html#section-test-correlation) we used a permutation test
    for the correlation between birth weight and mother’s age, and found that it is
    statistically significant, with p-value less than 0.001.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can do the same thing analytically. The method is based on this mathematical
    result: If we generate two samples with size `n` from normal distributions, compute
    Pearson’s correlation, `r`, and then transform the correlation with this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: The transformed correlations follow a \(t\) distribution with parameter `n-2`.
    To see what that looks like, we’ll use the following function to generate uncorrelated
    samples from a standard normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: And the following function to compute their correlation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: The following loop generates many pairs of samples, computes their correlation,
    and puts the results in a list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: Next we’ll compute the transformed correlations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: To check whether these `ts` follow a \(t\) distribution, we’ll use the following
    function, which makes an object that represents the CDF of a \(t\) distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: The parameter of the \(t\) distribution is called `df`, which stands for “degrees
    of freedom”. The following figure shows the CDF of a \(t\) distribution with parameter
    `n-2` along with the empirical CDF of the transformed correlations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/3f882ea0aeae5d9714d9e5958377a90cb38f09ce078584497fec4ff0d9b2223a.png](../Images/0027d4a9eb77d62362db11c50da43e55.png)'
  prefs: []
  type: TYPE_IMG
- en: This shows that if we draw uncorrelated samples from normal distributions, their
    transformed correlations follow a \(t\) distribution.
  prefs: []
  type: TYPE_NORMAL
- en: If we draw samples from other distributions, their transformed correlations
    don’t follow a \(t\) distribution exactly, but they converge to a \(t\) distribution
    as the sample size increases. Let’s see if this applies to the correlation of
    maternal age and birth weight. From the `DataFrame` of live births, we’ll select
    the rows with valid data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: The actual correlation is about 0.07.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: As we did in [Chapter 9](chap09.html#section-test-correlation), we can simulate
    the null hypothesis by permuting the samples.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: If we generate many permutations and compute their correlations, the result
    is a sample from the distribution of correlations under the null hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: And we can compute the transformed correlations like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: The following figure shows the empirical CDF of the `ts` along with the CDF
    of the \(t\) distribution with parameter `n-2`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/969c27dc5ddcfb66a7f3f7217e0c8e179ef642cbac9a8808c75b787e31aae984.png](../Images/f1e9af59bc2ada1cfa2f64be302a5ef5.png)'
  prefs: []
  type: TYPE_IMG
- en: The model fits the empirical distribution well, which means we can use it to
    compute a p-value for the observed correlation. First we’ll transform the observed
    correlation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: Now we can use the CDF of the \(t\) distribution to compute the probability
    of a value as large as `t_actual` under the null hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: We can also compute the probability of a value as negative as `-t_actual`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: The sum of the two is the probability of a correlation as big as `r_actual`,
    positive or negative.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: SciPy provides a function that does the same calculation and returns the p-value
    of the observed correlation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: The results are nearly the same.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the resampling results, we concluded that the p-value was less than
    0.001, but we could not say how much less without running a very large number
    of resamplings. With analytic methods, we can compute small p-values quickly.
  prefs: []
  type: TYPE_NORMAL
- en: However, in practice it might not matter. Generally, if a p-value is smaller
    than 0.001, we can conclude that the observed effect is unlikely to be due to
    chance. It is not usually important to know precisely how unlikely.
  prefs: []
  type: TYPE_NORMAL
- en: Chi-squared Test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 9](chap09.html#section-testing-proportions) we tested whether a
    die is crooked, based on this set of observed outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: '|  | freqs |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| outcome |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 9 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 19 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 11 |'
  prefs: []
  type: TYPE_TB
- en: First we computed the expected frequency for each outcome.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: Then we used the following function to compute the chi-squared statistic.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: The chi-squared statistic is widely used for this kind of data because its sampling
    distribution under the null hypothesis converges to a distribution we can compute
    efficiently – not coincidentally, it is called a **chi-squared distribution**.
    To see what it looks like, we’ll use the following function, which simulates rolling
    a fair die.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: The following loop runs the simulation many times and computes the chi-squared
    statistic of the outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: To check whether the results follow a chi-squared distribution, we’ll use the
    following function, which computes the CDF of a chi-squared distribution with
    parameter `df`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: With `n` possible outcomes, the simulated chi-squared statistics should follow
    a chi-squared distribution with parameter `n-1`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: Here’s the empirical CDF of the simulated chi-squared statistics along with
    the CDF of the chi-squared distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/14da013801f82e99c9c52b754fea106deff44e8452ea419fe232db08895628f6.png](../Images/8ba3e4fe75cc56ef21550a84fc7a7d37.png)'
  prefs: []
  type: TYPE_IMG
- en: The model fits the simulation results well, so we can use it to compute the
    probability of a value as large as `observed_chi2` under the null hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: SciPy provides a function that does the same computation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: The result is the same as the p-value we computed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: The advantage of the chi-squared statistic is that its distribution under the
    null hypothesis can be computed efficiently. But in context, it might not be the
    statistic that best quantifies the difference between the observed and expected
    outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Computation and Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This book focuses on computational methods like resampling and permutation.
    These methods have several advantages over analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: They are easier to explain and understand. For example, one of the most difficult
    topics in an introductory statistics class is hypothesis testing. Many students
    don’t really understand what p-values are. I think the approach we took in [Chapter
    9](chap09.html#chapter-hypothesis-testing) – simulating the null hypothesis and
    computing test statistics – makes the fundamental idea clearer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are robust and versatile. Analytic methods are often based on assumptions
    that don’t hold in practice. Computational methods require fewer assumptions,
    and can be adapted and extended more easily.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'They are debuggable. Analytic methods are often like a black box: you plug
    in numbers and they spit out results. But it’s easy to make subtle errors, hard
    to be confident that the results are right, and hard to diagnose the problem if
    they are not. Computational methods lend themselves to incremental development
    and testing, which fosters confidence in the results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But there are drawbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: Computational methods can be slow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomized methods like resampling don’t produce the same results every time,
    which makes it harder to check that they are correct.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taking into account these pros and cons, I recommend the following process:'
  prefs: []
  type: TYPE_NORMAL
- en: Use computational methods during exploration. If you find a satisfactory answer
    and the run time is acceptable, you can stop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If run time is not acceptable, look for opportunities to optimize. Using analytic
    methods is one of several methods of optimization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If replacing a computational method with an analytic method is appropriate,
    use the computational method as a basis of comparison, providing mutual validation
    between the computational and analytic results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For many practical problems, the run time of computational methods is not a
    problem, and we don’t have to go past the first step.
  prefs: []
  type: TYPE_NORMAL
- en: Glossary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**normal probability plot:** A plot that compares observed values with the
    quantiles of a normal distribution to see how closely the data follow a normal
    distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**independent sample \(t\) test:** A method for computing the p-value of an
    observed difference between the means of two independent groups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**\(t\) distribution:** A distribution used to model the sampling distribution
    of a difference in means under the null hypothesis that the difference is 0, as
    well as the sampling distribution of transformed correlations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**chi-squared distribution:** A distribution used to model the sampling distribution
    of the chi-squared statistic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**chi-squared statistic:** A test statistic that quantifies the magnitude of
    the difference between two discrete distributions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Exercise 14.1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter we compared the weights of male and female penguins and computed
    a confidence interval for the difference. Now let’s do the same for flipper length.
    The observed difference is about 4.6 mm.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: Use `sampling_dist_mean` to make `Normal` objects that represent sampling distributions
    for the mean flipper length in the two groups – noting that the groups are not
    the same size. Then compute the sampling distribution of the difference and a
    90% confidence interval.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 14.2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using the NSFG data, we computed the correlation between a baby’s birth weight
    and the mother’s age, and we used a \(t\) distribution to compute a p-value. Now
    let’s do the same with birth weight and father’s age, which is recorded in the
    `hpagelb` column.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: The observed correlation is about 0.065.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: Compute the transformed correlation, `t_actual`. Use the CDF of the \(t\) distribution
    to compute a p-value – is this correlation statistically significant? Use the
    SciPy function `pearsonr` to check your results.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 14.3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In one of the exercises in [Chapter 9](chap09.html#chapter-hypothesis-testing)
    we considered the Trivers-Willard hypothesis, which suggests that for many mammals
    the sex ratio depends on “maternal condition” – that is, factors like the mother’s
    age, size, health, and social status. Some studies have shown this effect among
    humans, but results are mixed.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, and a chance to practice a chi-squared test, let’s see if there’s
    a relationship between the sex of a baby and the mother’s marital status. The
    notebook for this chapter has instructions to help you get started.
  prefs: []
  type: TYPE_NORMAL
- en: First we’ll partition mothers of male and female babies.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we’ll make a `DataFrame` with one column for each group and one row for
    each value of `fmarital`, which encodes marital status like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: '|  | male | female |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| fmarital |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2576 | 2559 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 56 | 54 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 568 | 572 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 355 | 330 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1086 | 985 |'
  prefs: []
  type: TYPE_TB
- en: The null hypothesis is that the distribution of marital status is the same for
    both groups, so we can use the whole dataset to compute it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: '|  | probs |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| fmarital |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.561653 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.012024 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.124617 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.075208 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0.226498 |'
  prefs: []
  type: TYPE_TB
- en: To compute the expected values, we multiply the probabilities in `pmf_marital`
    by the total number of cases in each column.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: '|  | male | female |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| fmarital |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2606.630739 | 2527.437691 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 55.805641 | 54.110188 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 578.349366 | 560.778312 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 349.038916 | 338.434631 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1051.175339 | 1019.239178 |'
  prefs: []
  type: TYPE_TB
- en: 'Use `observed` and `expected` to compute a chi-squared statistic. Then use
    the CDF of the chi-squared distribution to compute a p-value. The degrees of freedom
    should be `n-1`, where `n` is the number of values in the observed `DataFrame`.
    Then use the SciPy function `chisquared` to compute the chi-squared statistic
    and p-value. Hint: use the argument `axis=None` to treat the entire `DataFrame`
    as a single test rather than one test for each column.'
  prefs: []
  type: TYPE_NORMAL
- en: Does this test provide support for the Trivers-Willard hypothesis?
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 14.4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The method we used in this chapter to analyze differences between groups can
    be extended to analyze “differences in differences”, which is a common experimental
    design. As an example, we’ll use data from a 2014 paper that investigates the
    effects of an intervention intended to mitigate gender-stereotypical task allocation
    within student engineering teams.
  prefs: []
  type: TYPE_NORMAL
- en: Stein, L. A., Aragon, D., Moreno, D., & Goodman, J. (2014, October). Evidence
    for the persistent effects of an intervention to mitigate gender-stereotypical
    task allocation within student engineering teams. In *2014 IEEE Frontiers in Education
    Conference (FIE) Proceedings* (pp. 1-9). IEEE.
  prefs: []
  type: TYPE_NORMAL
- en: Available from [http://ieeexplore.ieee.org/document/7044435/](http://ieeexplore.ieee.org/document/7044435/).
  prefs: []
  type: TYPE_NORMAL
- en: Before and after the intervention, students responded to a survey that asked
    them to rate their contribution to each aspect of class projects on a 7-point
    scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before the intervention, male students reported higher scores for the programming
    aspect of the projects than female students: men reported an average score of
    3.57 with standard error 0.28; women reported an average score of 1.91 with standard
    error 0.32.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After the intervention, the gender gap was smaller: the average score for men
    was 3.44 (SE 0.16); the average score for women was 3.18 (SE 0.16).'
  prefs: []
  type: TYPE_NORMAL
- en: Make four `Normal` objects to represent the sampling distributions of the estimated
    means before and after the intervention, for both male and female students. Because
    we have standard errors for the estimated means, we don’t need to know the sample
    size to get the parameters of the sampling distributions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the sampling distributions of the gender gap – the difference in means
    – before and after the intervention.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then compute the sampling distribution of the difference in differences – that
    is, the change in the size of the gap. Compute a 95% confidence interval and a
    p-value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is there evidence that the size of the gender gap decreased after the intervention?
  prefs: []
  type: TYPE_NORMAL
- en: '[Think Stats: Exploratory Data Analysis in Python, 3rd Edition](https://allendowney.github.io/ThinkStats/index.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Copyright 2024 [Allen B. Downey](https://allendowney.com)
  prefs: []
  type: TYPE_NORMAL
- en: 'Code license: [MIT License](https://mit-license.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Text license: [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International](https://creativecommons.org/licenses/by-nc-sa/4.0/)'
  prefs: []
  type: TYPE_NORMAL
