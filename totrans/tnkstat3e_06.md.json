["```py\nfrom  os.path  import basename, exists\n\ndef  download(url):\n    filename = basename(url)\n    if not exists(filename):\n        from  urllib.request  import urlretrieve\n\n        local, _ = urlretrieve(url, filename)\n        print(\"Downloaded \" + local)\n\ndownload(\"https://github.com/AllenDowney/ThinkStats/raw/v3/nb/thinkstats.py\") \n```", "```py\ntry:\n    import  empiricaldist\nexcept ImportError:\n    %pip install empiricaldist \n```", "```py\nimport  numpy  as  np\nimport  pandas  as  pd\nimport  matplotlib.pyplot  as  plt\n\nfrom  thinkstats  import decorate \n```", "```py\ndownload(\"https://github.com/AllenDowney/ThinkStats/raw/v3/nb/relay.py\")\ndownload(\n    \"https://github.com/AllenDowney/ThinkStats/raw/v3/data/Apr25_27thAn_set1.shtml\"\n) \n```", "```py\nfrom  relay  import read_results\n\nresults = read_results()\nresults.head() \n```", "```py\nspeeds = results[\"MPH\"].values \n```", "```py\nmy_result = results.query(\"Nettime == '42:44'\")\nmy_result \n```", "```py\nmy_speed = speeds[96] \n```", "```py\n(speeds <= my_speed).sum() \n```", "```py\nnp.int64(1537) \n```", "```py\n(speeds <= my_speed).mean() * 100 \n```", "```py\nnp.float64(94.12124923453766) \n```", "```py\ndef  percentile_rank(x, seq):\n  \"\"\"Percentile rank of x.\n\n x: value\n seq: sequence of values\n\n returns: percentile rank 0-100\n \"\"\"\n    return (seq <= x).mean() * 100 \n```", "```py\nmy_division = results.query(\"Division == 'M4049'\")\nmy_division_speeds = my_division[\"MPH\"].values \n```", "```py\npercentile_rank(my_speed, my_division_speeds) \n```", "```py\nnp.float64(90.234375) \n```", "```py\ndef  percentile(p, seq):\n    n = len(seq)\n    i = (1 - p / 100) * (n + 1)\n    return seq[round(i)] \n```", "```py\npercentile(90, my_division_speeds) \n```", "```py\nnp.float64(8.591885441527447) \n```", "```py\nnext_division = results.query(\"Division == 'M5059'\")\nnext_division_speeds = next_division[\"MPH\"].values\n\npercentile(90.2, next_division_speeds) \n```", "```py\nnp.float64(8.017817371937639) \n```", "```py\nnext_division.query(\"MPH > 8.01\").tail(1) \n```", "```py\nt = [1, 2, 2, 3, 5] \n```", "```py\nfrom  empiricaldist  import Pmf\n\npmf = Pmf.from_seq(t)\npmf \n```", "```py\npmf[2] \n```", "```py\nnp.float64(0.4) \n```", "```py\ncdf = pmf.make_cdf()\ncdf \n```", "```py\ncdf[2] \n```", "```py\nnp.float64(0.6000000000000001) \n```", "```py\ncdf(3) \n```", "```py\narray(0.8) \n```", "```py\ncdf(4) \n```", "```py\narray(0.8) \n```", "```py\ncdf.step()\ndecorate(xlabel=\"x\", ylabel=\"CDF\") \n```", "```py\nfrom  empiricaldist  import Cdf\n\ncdf_speeds = Cdf.from_seq(speeds) \n```", "```py\ncdf_speeds.step()\nplt.axvline(my_speed, ls=\":\", color=\"gray\")\ndecorate(xlabel=\"Speed (mph)\", ylabel=\"CDF\") \n```", "```py\ncdf_speeds(my_speed) * 100 \n```", "```py\nnp.float64(94.12124923453766) \n```", "```py\ncdf_speeds.inverse(0.5) \n```", "```py\narray(6.70391061) \n```", "```py\ndownload(\"https://github.com/AllenDowney/ThinkStats/raw/v3/nb/nsfg.py\")\ndownload(\"https://github.com/AllenDowney/ThinkStats/raw/v3/data/2002FemPreg.dct\")\ndownload(\"https://github.com/AllenDowney/ThinkStats/raw/v3/data/2002FemPreg.dat.gz\") \n```", "```py\ntry:\n    import  statadict\nexcept ImportError:\n    %pip install statadict \n```", "```py\nfrom  nsfg  import get_nsfg_groups\n\nlive, firsts, others = get_nsfg_groups() \n```", "```py\nfirst_weights = firsts[\"totalwgt_lb\"].dropna()\nfirst_weights.mean() \n```", "```py\nnp.float64(7.201094430437772) \n```", "```py\nother_weights = others[\"totalwgt_lb\"].dropna()\nother_weights.mean() \n```", "```py\nnp.float64(7.325855614973262) \n```", "```py\nfrom  empiricaldist  import Pmf\n\nfirst_pmf = Pmf.from_seq(first_weights, name=\"first\")\nother_pmf = Pmf.from_seq(other_weights, name=\"other\") \n```", "```py\nfrom  thinkstats  import two_bar_plots\n\ntwo_bar_plots(first_pmf, other_pmf, width=0.06)\ndecorate(xlabel=\"Weight (pounds)\", ylabel=\"PMF\") \n```", "```py\nfirst_cdf = first_pmf.make_cdf()\nother_cdf = other_pmf.make_cdf() \n```", "```py\nfirst_cdf.plot(ls=\"--\")\nother_cdf.plot(alpha=0.5)\ndecorate(xlabel=\"Weight (pounds)\", ylabel=\"CDF\") \n```", "```py\nfrom  nsfg  import read_stata\n\ndct_file = \"2002FemPreg.dct\"\ndat_file = \"2002FemPreg.dat.gz\"\n\npreg = read_stata(dct_file, dat_file) \n```", "```py\nbirthwgt_lb = preg[\"birthwgt_lb\"]\nbirthwgt_oz = preg[\"birthwgt_oz\"] \n```", "```py\nfrom  empiricaldist  import Hist\n\nHist.from_seq(birthwgt_oz).tail(5) \n```", "```py\nHist.from_seq(birthwgt_lb).tail(5) \n```", "```py\nbirthwgt_lb_clean = birthwgt_lb.replace([51, 97, 98, 99], np.nan)\nbirthwgt_oz_clean = birthwgt_oz.replace([97, 98, 99], np.nan)\n\ntotal_weight_clean = birthwgt_lb_clean + birthwgt_oz_clean / 16 \n```", "```py\ntotal_weight_bogus = birthwgt_lb + birthwgt_oz / 16 \n```", "```py\ncount1, count2 = total_weight_bogus.count(), total_weight_clean.count()\ndiff = count1 - count2\n\ndiff, diff / count2 * 100 \n```", "```py\n(np.int64(49), np.float64(0.5421553441026776)) \n```", "```py\nmean1, mean2 = total_weight_bogus.mean(), total_weight_clean.mean()\nmean1, mean2 \n```", "```py\n(np.float64(7.319680587652691), np.float64(7.265628457623368)) \n```", "```py\n(mean1 - mean2) / mean2 * 100 \n```", "```py\nnp.float64(0.74394294099376) \n```", "```py\nstd1, std2 = total_weight_bogus.std(), total_weight_clean.std()\nstd1, std2 \n```", "```py\n(np.float64(2.096001779161835), np.float64(1.4082934455690173)) \n```", "```py\n(std1 - std2) / std2 * 100 \n```", "```py\nnp.float64(48.83274403900607) \n```", "```py\ndef  skewness(seq):\n  \"\"\"Compute the skewness of a sequence\n\n seq: sequence of numbers\n\n returns: float skewness\n \"\"\"\n    deviations = seq - seq.mean()\n    return np.mean(deviations**3) / seq.std(ddof=0) ** 3 \n```", "```py\nskew1, skew2 = skewness(total_weight_bogus), skewness(total_weight_clean)\nskew1, skew2 \n```", "```py\n(np.float64(22.251846195422484), np.float64(-0.5895062687577697)) \n```", "```py\n# how much is skew1 off by?\n(skew1 - skew2) / skew2 \n```", "```py\nnp.float64(-38.74658112171128) \n```", "```py\ncdf_total_weight_bogus = Cdf.from_seq(total_weight_bogus)\ncdf_total_weight_clean = Cdf.from_seq(total_weight_clean) \n```", "```py\ndef  median(cdf):\n    m = cdf.inverse(0.5)\n    return m \n```", "```py\nmedian(cdf_total_weight_bogus), median(cdf_total_weight_clean) \n```", "```py\n(array(7.375), array(7.375)) \n```", "```py\ndef  iqr(cdf):\n    low, high = cdf.inverse([0.25, 0.75])\n    return high - low \n```", "```py\niqr(cdf_total_weight_bogus), iqr(cdf_total_weight_clean) \n```", "```py\n(np.float64(1.625), np.float64(1.625)) \n```", "```py\ndef  quartile_skewness(cdf):\n    low, median, high = cdf.inverse([0.25, 0.5, 0.75])\n    midpoint = (high + low) / 2\n    semi_iqr = (high - low) / 2\n    return (midpoint - median) / semi_iqr \n```", "```py\nqskew1 = quartile_skewness(cdf_total_weight_bogus)\nqskew2 = quartile_skewness(cdf_total_weight_clean)\nqskew1, qskew2 \n```", "```py\n(np.float64(-0.07692307692307693), np.float64(-0.07692307692307693)) \n```", "```py\ndef  sample_from_cdf(cdf, n):\n    ps = np.random.random(size=n)\n    return cdf.inverse(ps) \n```", "```py\nsample = sample_from_cdf(cdf_speeds, 1001) \n```", "```py\ncdf_sample = Cdf.from_seq(sample)\n\ncdf_speeds.plot(label=\"original\", ls=\"--\")\ncdf_sample.plot(label=\"sample\", alpha=0.5)\n\ndecorate(xlabel=\"Speed (mph)\", ylabel=\"CDF\") \n```", "```py\npercentile_ranks = cdf_speeds(sample) * 100 \n```", "```py\ncdf_percentile_rank = Cdf.from_seq(percentile_ranks)\ncdf_percentile_rank.plot()\n\ndecorate(xlabel=\"Percentile rank\", ylabel=\"CDF\") \n```", "```py\nsample = cdf_speeds.sample(1001) \n```", "```py\nfrom  nsfg  import get_nsfg_groups\n\nlive, firsts, others = get_nsfg_groups() \n```", "```py\nmale = live.query(\"babysex == 1\")\nfemale = live.query(\"babysex == 2\")\nlen(male), len(female) \n```", "```py\n(4641, 4500) \n```", "```py\nfrom  nsfg  import read_fem_preg\n\npreg = read_fem_preg() \n```", "```py\nspeeds = results[\"MPH\"].values \n```", "```py\nt = np.random.random(1001) \n```"]